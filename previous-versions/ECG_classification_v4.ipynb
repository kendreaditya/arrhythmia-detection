{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"colab":{"name":"CNN 1D.ipynb","provenance":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"G2aZOrVmF1JK","colab_type":"code","outputId":"473c5f07-775f-4991-9bed-2d7640073893","executionInfo":{"status":"ok","timestamp":1577747718029,"user_tz":300,"elapsed":8403,"user":{"displayName":"Aditya Kendre","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB35l7B61CodGQ3BObzLd6EEysUeGYqqcyf3VG4Sg=s64","userId":"02283309652414962644"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["# Run this for pre-processesed dataset file\n","! git clone https://github.com/kendreaditya/ECG_DATA.git"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cloning into 'ECG_DATA'...\n","remote: Enumerating objects: 4, done.\u001b[K\n","remote: Counting objects: 100% (4/4), done.\u001b[K\n","remote: Compressing objects: 100% (3/3), done.\u001b[K\n","remote: Total 4 (delta 0), reused 4 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects: 100% (4/4), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FHuFXF_8Fxt9","colab_type":"code","colab":{}},"source":["import pickle\n","import random\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim \n","from torch.autograd import Variable"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4ig5SfEBFxuB","colab_type":"code","colab":{}},"source":["# Number of words for which we are storing an embedding\n","vocab_size    = 3000\n","# Number of dimension of the embeddings\n","embedding_dim = 50\n","batch_size    = 256\n","input_len     = 36\n","epochs        = 10\n","print_every   = 1000\n","cuda          = True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VtRGugv8FxuP","colab_type":"code","colab":{}},"source":["def load_files():\n","    with open('ECG_DATA/', 'rb') as data_file:\n","        data = pickle.load(data_file)\n","\n","    with open('../data/sentiment_vocabulary.pkl', 'rb') as vocab_file:\n","        vocab = pickle.load(vocab_file)\n","        \n","    return data, vocab"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cVCWkrVrFxuT","colab_type":"text"},"source":["This function creates a correspondance between the `vocab_size` most frequent words and integers from $[1, \\text{vocab_size}]$. We will be using the index 0 to encode rare words and to pad the word sequences to an unique lenghts."]},{"cell_type":"code","metadata":{"id":"kqx6wE7TFxuU","colab_type":"code","colab":{}},"source":["def create_word_to_idx(vocab):\n","    items       = list(vocab.items())\n","    items       = sorted(items, key = lambda x: x[1], reverse = True)\n","    word_to_idx = {word : i + 1 for i, (word, _) in enumerate(items[:vocab_size])}\n","    \n","    return word_to_idx"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"57IxsKwGFxuW","colab_type":"code","colab":{}},"source":["def encode_data(data, word_to_idx, input_len):\n","    encoded_data = []\n","    \n","    # For each tweet, we compute the sequence of indices corresponding to \n","    # its list of words. If the length of this sequence is smaller than \n","    # input_len words, we pad it with zeros. If the sequence is longer, we \n","    # cut it down to input_len words. \n","    for tweet, target in data:\n","        encoded_tweet = [word_to_idx.get(word, 0) for word in tweet]\n","        len_encoding  = len(encoded_tweet) \n","        if len(encoded_tweet) < input_len:\n","            encoded_tweet = encoded_tweet + [0] * (input_len - len_encoding)\n","        else:\n","            encoded_tweet = encoded_tweet[:input_len]\n","        encoded_data.append((' '.join(tweet), encoded_tweet, target))\n","        \n","    return encoded_data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xyMGUP1nFxuY","colab_type":"code","colab":{}},"source":["def load_data(vocab_size, input_len, test_proportion = 0.2):\n","    data, vocab   = load_files()\n","    word_to_idx   = create_word_to_idx(vocab)\n","    encoded_data  = encode_data(data, word_to_idx, input_len)\n","    # We split the data into a training set and a test set.\n","    training_size = int(len(encoded_data) * (1 - test_proportion))  \n","    random.shuffle(encoded_data)\n","    training_data = encoded_data[:training_size]\n","    test_data     = encoded_data[training_size:]\n","    \n","    return training_data, test_data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZtWfx6rtFxuc","colab_type":"code","colab":{}},"source":["def batch_to_tensor(batch):\n","    tweets  = [tweet for tweet, _, _ in batch]\n","    inputs  = torch.LongTensor([input for _, input, _ in batch])\n","    targets = torch.LongTensor([target for _, _, target in batch])\n","    \n","    return tweets, inputs, targets"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NsDnD-5TFxug","colab_type":"code","colab":{}},"source":["def batch_generator(data, batch_size, shuffle = True):\n","    if shuffle:\n","        data = random.sample(data, len(data))\n","        \n","    return (batch_to_tensor(data[i: i + batch_size]) for i in range(0, len(data), batch_size))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gbStyierFxui","colab_type":"code","colab":{}},"source":["def evaluate_model(cnn, criterion, train_data, test_data, batch_size):\n","    def evaluate_model_data(data):\n","        batch_number     = 0\n","        total_loss       = 0\n","        total_correct    = 0\n","        total_prediction = 0\n","        for _, inputs, targets in batch_generator(data, batch_size, shuffle = False):\n","            inputs            = Variable(inputs)\n","            targets           = Variable(targets)\n","            inputs            = inputs.cuda() if cuda else inputs\n","            targets           = targets.cuda() if cuda else targets\n","            predictions       = cnn(inputs)\n","            loss              = criterion(predictions, targets)\n","            total_loss       += loss.cpu().data[0]\n","            batch_number     += 1\n","            pred_classes      = predictions.max(dim = 1)[1]\n","            total_prediction += predictions.size()[0]\n","            total_correct    += (pred_classes == targets).cpu().sum().data[0]\n","        average_loss     = total_loss / batch_number\n","        average_accuracy = total_correct / total_prediction\n","        \n","        return average_loss, average_accuracy\n","    \n","    return evaluate_model_data(train_data), evaluate_model_data(test_data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"biXqUQX3Fxum","colab_type":"code","colab":{}},"source":["def print_model_evaluation(cnn, epoch, criterion, train_data, test_data, batch_size):\n","    cnn.eval()\n","    evaluation = evaluate_model(cnn, criterion, train_data, test_data, batch_size)\n","    cnn.train()\n","    print(\n","        f'[{epoch + 1:3}] ' \n","        f'train loss: {evaluation[0][0]:.4f}, train accuracy: {100 * evaluation[0][1]:.3f}%, '\n","        f'test loss: {evaluation[1][0]:.4f}, test accuracy: {100 * evaluation[1][1]:.3f}%'\n","    )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oyFHeCCZFxup","colab_type":"text"},"source":["The network used in this notebook is a simple CNN with Batch Normalization and Dropout. In the forward pass, after computing the embedding of each word of the sequence, we have to permute the last two dimensions as the embedding layer outputs has a `(batch_size, seq_len, channels)` shape and the convolutions layers take `(batch_size, channels, seq_len)` has input shape. More details on this issue can be found in [this topic](https://discuss.pytorch.org/t/inconsistent-dimension-ordering-for-1d-networks-ncl-vs-nlc-vs-lnc/14807)."]},{"cell_type":"code","metadata":{"id":"g77MWTprFxuq","colab_type":"code","colab":{}},"source":["class CNN(nn.Module):\n","    def __init__(self, vocab_size, input_len, embedding_dim):\n","        super(CNN, self).__init__()\n","        self.conv1         = nn.Conv1d(1, 64, 3, padding = 1)\n","        self.bn1           = nn.BatchNorm1d(64)\n","        self.dropout1      = nn.Dropout(p = 0.8)\n","        self.conv2         = nn.Conv1d(64 , 64 , 3, padding = 1)\n","        self.bn2           = nn.BatchNorm1d(64)\n","        self.dropout2      = nn.Dropout(p = 0.8)\n","        self.conv3         = nn.Conv1d(64 , 128, 3, padding = 1)\n","        self.bn3           = nn.BatchNorm1d(128)\n","        self.dropout3      = nn.Dropout(p = 0.8)\n","        self.conv4         = nn.Conv1d(128, 128, 3, padding = 1)\n","        self.bn4           = nn.BatchNorm1d(128)\n","        self.dropout4      = nn.Dropout(p = 0.8)\n","        self.linear1       = nn.Linear(128 * 9, 256)\n","        self.bn5           = nn.BatchNorm1d(256)\n","        self.dropout5      = nn.Dropout(p = 0.8)\n","        self.linear2       = nn.Linear(256, 256)\n","        self.bn6           = nn.BatchNorm1d(256)\n","        self.dropout6      = nn.Dropout(p = 0.8)\n","        self.linear3       = nn.Linear(256, 4)\n","        \n","    def forward(self, x):\n","        x = self.embedding(x)\n","        x = x.transpose(1, 2).contiguous()\n","        x = self.dropout1(self.bn1(F.relu(self.conv1(x))))\n","        x = self.dropout2(self.bn2(F.relu(self.conv2(x))))\n","        x = F.avg_pool1d(x, 2)\n","        x = self.dropout3(self.bn3(F.relu(self.conv3(x))))\n","        x = self.dropout4(self.bn4(F.relu(self.conv4(x))))\n","        x = F.avg_pool1d(x, 2)\n","        x = x.view(-1, 9 * 128)\n","        x = self.dropout5(self.bn5(F.relu(self.linear1(x))))\n","        x = self.dropout6(self.bn6(F.relu(self.linear2(x))))\n","        x = F.log_softmax(self.linear3(x), dim = 1)\n","        \n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WCVerfb9Fxus","colab_type":"code","colab":{}},"source":["train_data, test_data = load_data(vocab_size, input_len)\n","cnn                   = CNN(vocab_size, input_len, embedding_dim)\n","cnn                   = cnn.cuda() if cuda else cnn\n","criterion             = nn.NLLLoss()\n","optimizer             = optim.Adam(cnn.parameters())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lnpUGsiYFxuu","colab_type":"code","outputId":"35d999cf-151f-4251-b68a-29406a7c6b7e","colab":{}},"source":["print_model_evaluation(cnn, 0, criterion, train_data, test_data, batch_size)\n","for epoch in range(epochs):\n","    total_loss   = 0\n","    running_loss = 0\n","    for i, (_, inputs, targets) in enumerate(batch_generator(train_data, batch_size)):\n","        optimizer.zero_grad()\n","        inputs        = Variable(inputs)\n","        targets       = Variable(targets)\n","        inputs        = inputs.cuda() if cuda else inputs\n","        targets       = targets.cuda() if cuda else targets\n","        predictions   = cnn(inputs)\n","        loss          = criterion(predictions, targets)\n","        loss_value    = loss.cpu().data[0]\n","        running_loss += loss_value\n","        loss.backward()\n","        optimizer.step()\n","        \n","        if i % print_every == print_every - 1:\n","            print(f'\\t[{i + 1:6}] running_loss: {running_loss / print_every:.4f}')\n","            running_loss = 0\n","\n","    print_model_evaluation(cnn, epoch + 1, criterion, train_data, test_data, batch_size)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[  1] train loss: 0.6943, train accuracy: 50.077%, test loss: 0.6944, test accuracy: 49.967%\n","\t[  1000] running_loss: 0.6972\n","\t[  2000] running_loss: 0.6430\n","\t[  3000] running_loss: 0.5614\n","\t[  4000] running_loss: 0.5199\n","[  2] train loss: 0.4674, train accuracy: 78.343%, test loss: 0.4769, test accuracy: 77.699%\n","\t[  1000] running_loss: 0.4779\n","\t[  2000] running_loss: 0.4718\n","\t[  3000] running_loss: 0.4652\n","\t[  4000] running_loss: 0.4615\n","[  3] train loss: 0.4166, train accuracy: 81.428%, test loss: 0.4396, test accuracy: 79.930%\n","\t[  1000] running_loss: 0.4417\n","\t[  2000] running_loss: 0.4381\n","\t[  3000] running_loss: 0.4369\n","\t[  4000] running_loss: 0.4373\n","[  4] train loss: 0.3918, train accuracy: 82.704%, test loss: 0.4296, test accuracy: 80.348%\n","\t[  1000] running_loss: 0.4194\n","\t[  2000] running_loss: 0.4191\n","\t[  3000] running_loss: 0.4222\n","\t[  4000] running_loss: 0.4203\n","[  5] train loss: 0.3772, train accuracy: 83.527%, test loss: 0.4271, test accuracy: 80.448%\n","\t[  1000] running_loss: 0.4068\n","\t[  2000] running_loss: 0.4052\n","\t[  3000] running_loss: 0.4074\n","\t[  4000] running_loss: 0.4065\n","[  6] train loss: 0.3655, train accuracy: 84.128%, test loss: 0.4292, test accuracy: 80.508%\n","\t[  1000] running_loss: 0.3913\n","\t[  2000] running_loss: 0.3948\n","\t[  3000] running_loss: 0.3984\n","\t[  4000] running_loss: 0.3992\n","[  7] train loss: 0.3561, train accuracy: 84.615%, test loss: 0.4312, test accuracy: 80.456%\n","\t[  1000] running_loss: 0.3839\n","\t[  2000] running_loss: 0.3874\n","\t[  3000] running_loss: 0.3891\n","\t[  4000] running_loss: 0.3924\n","[  8] train loss: 0.3498, train accuracy: 84.957%, test loss: 0.4327, test accuracy: 80.380%\n","\t[  1000] running_loss: 0.3754\n","\t[  2000] running_loss: 0.3796\n","\t[  3000] running_loss: 0.3834\n","\t[  4000] running_loss: 0.3839\n","[  9] train loss: 0.3420, train accuracy: 85.396%, test loss: 0.4357, test accuracy: 80.338%\n","\t[  1000] running_loss: 0.3704\n","\t[  2000] running_loss: 0.3734\n","\t[  3000] running_loss: 0.3781\n","\t[  4000] running_loss: 0.3781\n","[ 10] train loss: 0.3358, train accuracy: 85.695%, test loss: 0.4395, test accuracy: 80.183%\n","\t[  1000] running_loss: 0.3641\n","\t[  2000] running_loss: 0.3695\n","\t[  3000] running_loss: 0.3701\n","\t[  4000] running_loss: 0.3736\n","[ 11] train loss: 0.3313, train accuracy: 85.926%, test loss: 0.4444, test accuracy: 80.100%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tir1n3XXFxux","colab_type":"text"},"source":["We can see that the network overfits the training data quite a lot even using dropout."]}]}